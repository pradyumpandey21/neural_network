# -*- coding: utf-8 -*-
"""neuralnetworks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OhjaZaO9C_fjBvWVfb3EiWjIzoFODsDD
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as pit

#connecting colab to gdrive
from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/digit-recognizer/train.csv')

data.head()

data =np.array(data)
m, n= data.shape
np.random.shuffle(data)

print(m,n)

train_data=data[0:int(0.8*m), :]
val_data =data[int(0.8*m):m, :]

X_train = train_data[:, 1:].T
X_train = X_train / 255.0
Y_train = train_data[:, 0]

X_val = val_data[:, 1:].T
X_val = X_val / 255.0
Y_val = val_data[:, 0]

print(X_val.shape)
print()

from binascii import b2a_base64
def initialize_parameters():
  # He initialization for W1 (input to ReLU layer)
  W1 = np.random.randn(10, 784) * np.sqrt(2./784)
  B1 = np.zeros((10, 1)) # Biases initialized to zero

  # He initialization for W2 (ReLU layer to output)
  W2 = np.random.randn(10, 10) * np.sqrt(2./10)
  B2 = np.zeros((10, 1)) # Biases initialized to zero
  return W1 , B1 , W2, B2

def ReLU(X):
  return np.maximum(X, 0)

def softmax_calculator(Z):
  return np.exp(Z) / np.sum(np.exp(Z), axis=0)


def forward_propagation(W1, B1, W2, B2, X):
  Z1 = W1.dot(X) + B1
  A1 = ReLU(Z1)
  Z2 = W2.dot(A1) + B2
  A2 = softmax_calculator(Z2)
  return Z1, A1, Z2, A2

def one_hot_converter(Y):
  one_hot_Y = np.zeros((Y.size, Y.max() + 1))
  one_hot_Y[np.arange(Y.size), Y] = 1
  return one_hot_Y.T

def back_propagation(W1 , B1 , W2, B2, Z1, A1, Z2, A2,X, Y):
  current_m = X.shape[1]
  one_hot_Y = one_hot_converter(Y)
  dZ2 = A2 - one_hot_Y
  dW2 = 1 / current_m * dZ2.dot(A1.T)
  dB2 = 1 / current_m * np.sum(dZ2, axis=1, keepdims=True)
  dZ1 = W2.T.dot(dZ2)* (Z1 > 0)
  dW1 = 1 / current_m * dZ1.dot(X.T)
  dB1 = 1 / current_m * np.sum(dZ1, axis=1, keepdims=True)

  return dW1, dB1, dW2, dB2

def update_parameters(W1, B1, W2, B2, dW1, dB1, dW2, dB2, learning_rate ):
  W1 = W1 - learning_rate * dW1
  B1 = B1 - learning_rate * dB1
  W2 = W2 - learning_rate * dW2
  B2 = B2 - learning_rate * dB2
  return W1, B1, W2, B2

def get_predictions(A2):
  return np.argmax(A2, 0)

def get_accuracy(predictions, Y):

  return np.sum(predictions == Y) / Y.size

def gradient_descent(X, Y, alpha, iterations):
  W1, B1, W2, B2 = initialize_parameters()

  for i in range(iterations):

    Z1, A1, Z2, A2 = forward_propagation(W1, B1, W2, B2, X)
    dW1, dB1, dW2, dB2 = back_propagation(W1 , B1 , W2, B2, Z1, A1, Z2, A2,X, Y)
    W1, B1, W2, B2 = update_parameters(W1 , B1 , W2, B2,dW1, dB1, dW2, dB2, alpha)

    if (i%20)==0:
      print("Iteration: ", i)
      print("Accuracy: ", get_accuracy(get_predictions(A2), Y))
  return W1, B1, W2, B2

W1, B1, W2, B2 = gradient_descent(X_train, Y_train, 0.1 , 100)

val_index =0
Z1val, A1val, Z2val, A2val = forward_propagation(W1, B1, W2, B2, X_val[:, val_index, None ])
print("predicted label :", get_predictions(A2val))
print("actual label :", Y_val[val_index])

image_array = X_val[:,val_index].reshape(28,28)
pit.imshow(image_array, cmap='gray')
pit.show()

Z1val, A1val, Z2val, A2val = forward_propagation(W1, B1, W2, B2, X_val)
val_acc = get_accuracy(get_predictions(A2val), Y_val)
print("Validation accuracy =", val_acc )